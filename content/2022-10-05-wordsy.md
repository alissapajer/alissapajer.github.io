title: All 5x5 Grids of English Words
tags: programming

Here's an example of a valid 5x5 grid. It contains 10 English words (5 across words and 5 down words).

```
a w a r e
m o l a l
a v o i d
s e n s e
s n e e r
```

This project aims to find all 5x5 grids of English words. I'm using the python library `from english_words import english_words_lower_alpha_set`. This set contains 3213 words. I removed the majority of the proper nouns and all of the non-words (I hope!), which brings the word count down to 2481.

There are ${2481 choose 5 = 7.8 x 10^14} possible combinations of 5 words. If we can process 100 combinations per second, then we'd need about 250,000 years to compute the set of all grids. This is back-of-the-napkin math, but it gives us a general sense of the orders of magnitudes we're dealing with here.

Here is a naive implementation that finds all valid 5x5 grids.

```python
from tqdm.notebook import tqdm

def find_grids_slow(words):
    words2 = set(words) # checking set containment is faster than checking list containment
    for a0 in words:
        for a1 in words:
            for a2 in tqdm(words):
                for a3 in words:
                    for a4 in words:
                        d0 = a0[0] + a1[0] + a2[0] + a3[0] + a4[0]
                        d1 = a0[1] + a1[1] + a2[1] + a3[1] + a4[1]
                        d2 = a0[2] + a1[2] + a2[2] + a3[2] + a4[2]
                        d3 = a0[3] + a1[3] + a2[3] + a3[3] + a4[3]
                        d4 = a0[4] + a1[4] + a2[4] + a3[4] + a4[4]
                        if (d0 in words2) and (d1 in words2) and (d2 in words2) and (d3 in words2) and (d4 in words2):
                            grid = "\n".join((a0,a1,a2,a3,a4,"\n"))
                            return grid
```

I ran this function using a set of 3069 words. For `a0 = "sloth"`, it took 9 hours to run for each of the first two values of `a2`. (I didn't print out what these two values were). There are 3069 possible values for `a2`. So, 9 * 3069 = 27621 hours to complete the computation for all values of `a2`. Then we need to run that block for each value of `a1` and each value of `a0`, bringing us to 260,155,597,581 hours, which is 29.7 million years. Not tenable. Also, we're processing around 3 grids per second, which is surprisingly slow. Even if we improved this by a few orders of magnitude and ran it in parallel, the brute force approach isn't going to be viable.

Starting out, I had a poor intuition for how uncommon 5x5 grids are. While it would be nice to compute all grids, a few hundred grids would suffice for my use case. So, I decided to start out by trying to compute just one grid. I fixed `a0`. Then I chose a random `d0` (starting with the required prefix). Then I chose a random `a1`. Then a random `d1`. Then a random `a2`. The grid looked like this.

```
s t e e r
i r a t e
l a p s e
k c _ _ _
y e _ _ _
```

There are no English words that can complete this grid. I ran this function enough times to convince myself that statistically I'd never find a complete grid using this method. And I definitely wouldn't find hundreds (if there even are hundreds!). 

Next I experimented with fixing `a0`, and then iterating over all valid `d0`s, for all valid `a1` words, for all valid `d1` words, etc. The indexing was kind of unweidly, and I realized I wasn't gaining anything from alternating between across words and down words.

So, I decided to fix `a0`. Then iterate over each `a1` such that there exist valid down words. Then iterate over each `a2` such that there exist valid down words. After running this functiion I noticed that at this step there were often only a handlful of valid down words left. So, instead of iterating over the full set two more times (once for each of `a3` and `a4`), we can just consider the valid down words, and check that the `a3` and `a4` they produce are valid.

``
s t o v e
h a v e n
a l e r t
_ _ _ _ _ 
_ _ _ _ _ 
```

```python
from tqdm.notebook import tqdm

def find_grids(a0_words, all_words, output_file):
    all_words_set = set(all_words)
    for a0 in a0_words:
        print('a0:', a0)
        for a1 in tqdm(all_words_set - {a0}):
            p0 = a0[0] + a1[0]
            p1 = a0[1] + a1[1]
            p2 = a0[2] + a1[2]
            p3 = a0[3] + a1[3]
            p4 = a0[4] + a1[4]
            if not(has_prefix(p0, all_words_set) and has_prefix(p1, all_words_set) and has_prefix(p2, all_words_set) and has_prefix(p3, all_words_set) and has_prefix(p4, all_words_set)):
                continue
            for a2 in all_words_set - {a0,a1}:
                p0 = a0[0] + a1[0] + a2[0]
                p1 = a0[1] + a1[1] + a2[1]
                p2 = a0[2] + a1[2] + a2[2]
                p3 = a0[3] + a1[3] + a2[3]
                p4 = a0[4] + a1[4] + a2[4]
                # precomputing downs rather than filtering by prefix first is maybe faster
                d0s = filter_prefix(p0, all_words_set - {a0,a1,a2})
                d1s = filter_prefix(p1, all_words_set - {a0,a1,a2})
                d2s = filter_prefix(p2, all_words_set - {a0,a1,a2})
                d3s = filter_prefix(p3, all_words_set - {a0,a1,a2})
                d4s = filter_prefix(p4, all_words_set - {a0,a1,a2})
                if not((len(d0s) > 0) and (len(d1s) > 0) and (len(d2s) > 0) and (len(d3s) > 0) and (len(d4s) > 0)):
                    continue
                for d0 in d0s:
                    for d1 in d1s:
                        for d2 in d2s:
                            for d3 in d3s:
                                for d4 in d4s:
                                    a3 = d0[3] + d1[3] + d2[3] + d3[3] + d4[3]
                                    a4 = d0[4] + d1[4] + d2[4] + d3[4] + d4[4]
                                    if (a3 in all_words_set - {d0,d1,d2,d3,d4,a0,a1,a2}) and (a4 in all_words_set - {d0,d1,d2,d3,d4,a0,a1,a2,a3}):
                                        grid = "\n".join((a0,a1,a2,a3,a4,"\n"))
                                        with open(output_file, 'a') as file:
                                            file.write(grid)
```

This function is written such that I can run it in parallel (each run in a different jupyter notebook tab) by passing in a subset of the full word set for `a0_words`.

I could improve this roughly 2x by not double computing grid transposes, but that doesn't seem like a big enough win given the work involved.

I'm running `find_grids` in 8 separate jupyter notebook tabs. It is currently averaging 10 minutes per `a0`. That brings us to ${10 \times 2481 = 24810} minutes, which is 413 hours. ${413 / 8 = 51} hours. So, I expect to have all the grids within 3 days.

Thinking about this for a minute, it's possible that looking at all valid down words after fixing just `a0` and `a1` would be even faster. It will remove a loop of size 2481, and it will add additional `a2` checks. Maybe I'll try making that change the next time one of the loops completes; I'm curious. Though wile I could further improve the performance even further, it's fast enough right now.

If that is faster, then a takeaway is to think about ways to remove full loops and replace them loops over fewer things.

Optimization doesn't need to make things as fast as possible; it just needs to make things fast enough.
